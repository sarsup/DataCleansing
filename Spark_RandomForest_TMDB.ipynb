{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructField, StructType,Row,DateType, LongType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from StringIO import StringIO\n",
    "import csv\n",
    "import json\n",
    "import ast\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse record with both json and normal fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseRecord(data, schema):\n",
    "    rec = StringIO(data.encode('utf-8').strip())\n",
    "    reader = csv.reader(rec,delimiter=',', quotechar='\"')\n",
    "    row = reader.next()\n",
    "    rdict = {}\n",
    "    size = len(schema)\n",
    "    for i,field in enumerate(row):\n",
    "        if i<size:\n",
    "            rdict[schema[i]] = field\n",
    "    if i<size-1:\n",
    "        for ind in range(i+1,size):\n",
    "            print ind\n",
    "            rdict[schema[ind]]='0'\n",
    "    return Row(**rdict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNames(vec):\n",
    "    theList = ast.literal_eval(vec.strip())\n",
    "    res=''\n",
    "    for aDict in theList:\n",
    "        res+=aDict['name']+\"|\"\n",
    "    return res[:-1]\n",
    "\n",
    "def getDate(string):\n",
    "    try:\n",
    "        dt = datetime.strptime(string, '%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        dt = datetime.strptime('1900-01-01', '%Y-%m-%d')\n",
    "\n",
    "def getActor(jsonData,index):\n",
    "    res = \"\"\n",
    "    theList = ast.literal_eval(jsonData)\n",
    "    for k in theList:\n",
    "        if k['order'] == index:\n",
    "            res = k['name']\n",
    "            return res\n",
    "    return \"unknown\"\n",
    "\n",
    "def makeGetActor(num):\n",
    "    return udf(lambda x: getActor(x,num))\n",
    "\n",
    "def getDirector(stringdata):\n",
    "    data = ast.literal_eval(stringdata)\n",
    "    directors = [crewMember['name'] for crewMember in data if crewMember['job'] == 'Director']\n",
    "    if directors:\n",
    "        return directors[0]\n",
    "    else:\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register udfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getNames_udf = udf(getNames)\n",
    "getDate_udf = udf(getDate,DateType())\n",
    "getDirector_udf = udf(getDirector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieRDD = sc.textFile('tmdb_5000_movies.csv')\n",
    "columns = movieRDD.take(1)[0].split(\",\")\n",
    "movieDF = movieRDD.filter(lambda x:x.startswith('budget')==False).map(lambda x:parseRecord(x,columns)).toDF()\n",
    "movieDF = movieDF.withColumn('genre_string', getNames_udf(movieDF.genres)).withColumn('keywords_string', getNames_udf(movieDF.keywords))\n",
    "movieDF = movieDF.withColumn('prd_companies',getNames_udf(movieDF.production_companies))\n",
    "movieDF = movieDF.withColumn('releasedate',getDate_udf(movieDF.release_date)).withColumn('languages',getNames_udf(movieDF.spoken_languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newSchema = StructType(\n",
    "[\n",
    " StructField('budget',LongType(),True), StructField('id',IntegerType(),True), StructField('original_language',StringType(),True), \n",
    " StructField('original_title', StringType(),True), StructField('popularity', DoubleType(),True), StructField('revenue',DoubleType(),True), \n",
    " StructField('runtime', IntegerType(), True),StructField('status', StringType(),True), \n",
    " StructField('vote_average',DoubleType(), True),StructField('vote_count',IntegerType(),True),StructField('genre_string', StringType(),True), \n",
    " StructField('keywords_string',StringType(),True),StructField('prd_companies',StringType(),True),StructField('releasedate',DateType(),True),\n",
    " StructField('languages',StringType(),True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies = movieDF.select([movieDF['budget'].cast(LongType()), movieDF['id'].cast(IntegerType()) , \n",
    "                         'original_language', 'original_title', movieDF['popularity'].cast(DoubleType()) , \n",
    "                         movieDF['revenue'].cast(DoubleType()), movieDF['runtime'].cast(IntegerType()), \n",
    "                         'status', movieDF['vote_average'].cast(DoubleType()), movieDF['vote_count'].cast(IntegerType()), \n",
    "                         'genre_string', 'keywords_string', 'prd_companies', 'releasedate', \n",
    "                         'languages']).rdd.toDF(schema=newSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "creditsRDD = sc.textFile('tmdb_5000_credits.csv')\n",
    "columns = creditsRDD.take(1)[0].split(\",\")\n",
    "creditsDF = creditsRDD.filter(lambda x : x.startswith('movie_id')==False).map(lambda x: parseRecord(x,columns)).toDF()\n",
    "creditsDF = creditsDF.withColumn('actor1', makeGetActor(0)(creditsDF.cast)).withColumn('actor2', makeGetActor(1)(creditsDF.cast))\n",
    "creditsDF = creditsDF.withColumn('actor3', makeGetActor(2)(creditsDF.cast)).withColumn('director', getDirector_udf(creditsDF.crew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieCredits = creditsDF.select(creditsDF.movie_id.cast(IntegerType()), creditsDF.title,creditsDF.director,\n",
    "                               creditsDF.actor1, creditsDF.actor2,creditsDF.actor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join movies and movieCredits on movieID to get director and actor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = movies.join(movieCredits, movies.id == movieCredits.movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer,VectorAssembler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strcols = [ k for k,v in combined.dtypes if v == 'string']\n",
    "\n",
    "for col in strcols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col+\"_val\")\n",
    "    combined = indexer.fit(combined).transform(combined)\n",
    "combined = combined.na.fill(0)\n",
    "numerics = [k for k,v in combined.dtypes if v not in ['string', 'date']]\n",
    "numerics.remove('revenue')\n",
    "assembler = VectorAssembler(inputCols=numerics, outputCol='features')\n",
    "final_data = assembler.transform(combined).select('features','revenue')\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')\n",
    "final_data = scaler.fit(final_data).transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(labelCol='revenue', numTrees=150, featuresCol='scaledFeatures')\n",
    "train_data,test_data = final_data.randomSplit([0.7,0.3])\n",
    "rfcModel = rfc.fit(train_data)\n",
    "rfcpreds = rfcModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+\n",
      "|            features|      revenue|      scaledFeatures|          prediction|\n",
      "+--------------------+-------------+--------------------+--------------------+\n",
      "|[0.0,1591.0,3.481...|          0.0|[0.0,0.0179379550...|1.0006087427440144E7|\n",
      "|[0.0,63574.0,5.75...|          0.0|[0.0,0.7167740757...|   5843089.128004328|\n",
      "|[0.0,125052.0,0.3...|          0.0|[0.0,1.4099165024...|   5512645.393062637|\n",
      "|[4.0,68202.0,0.28...|          0.0|[9.82260588432448...|    4036847.31713399|\n",
      "|[2000000.0,92182....|  1.3101672E7|[0.04911302942162...|1.1125657196603218E7|\n",
      "|[6500000.0,11033....|     3.1899E7|[0.15961734562027...|1.6692682731684936E7|\n",
      "|[2.4E7,4935.0,49....| 2.34710455E8|[0.58935635305946...|1.3325200120975776E8|\n",
      "|[2.8E7,11141.0,6....|  3.0016165E7|[0.68758241190271...|2.6272622002093524E7|\n",
      "|[4.7E7,8592.0,7.8...| 1.03738726E8|[1.15415619140812...| 4.310695453091238E7|\n",
      "|[2.8E8,99861.0,13...|1.405403694E9|[6.87582411902713...| 9.798160673080947E8|\n",
      "|[7000000.0,24985....|          0.0|[0.17189560297567...|   5152077.101042667|\n",
      "|[1.2E7,3179.0,12....|          0.0|[0.29467817652973...|2.3607960794107728E7|\n",
      "|[3.0E7,22970.0,73...|   6.648608E7|[0.73669544132433...| 1.548833242432162E8|\n",
      "|[0.0,48035.0,3.22...|          0.0|[0.0,0.5415774173...|   8447451.136591775|\n",
      "|[700000.0,302579....|      10508.0|[0.01718956029756...|    6234118.16337508|\n",
      "|[2000000.0,331190...|    2801508.0|[0.04911302942162...|   8005517.101070199|\n",
      "|[5000000.0,36819....|  4.2365581E7|[0.12278257355405...| 2.991996872040465E7|\n",
      "|[3.0E7,15655.0,9....|  4.5554533E7|[0.73669544132433...| 2.434974213484807E7|\n",
      "|[0.0,13898.0,1.19...|          0.0|[0.0,0.1566949712...|    4262975.72216662|\n",
      "|[0.0,18828.0,1.90...|          0.0|[0.0,0.2122789552...|   5569864.563650552|\n",
      "+--------------------+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfcpreds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(19, {0: 0.2859, 1: 0.0153, 2: 0.1695, 3: 0.0355, 4: 0.0157, 5: 0.3477, 6: 0.0104, 7: 0.0001, 8: 0.0133, 10: 0.0085, 11: 0.0085, 12: 0.0227, 13: 0.0079, 14: 0.0123, 15: 0.0168, 16: 0.0117, 17: 0.0091, 18: 0.0091})"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfcModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
